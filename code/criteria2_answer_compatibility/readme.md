### This is the script to automatic evaluate criteria 2 (Answer Compatibility)

### before running you may want to set your openai keys for gpt-answer-base and gpt-score-base

### label 1 - Direct and Explicit, label 2 - Unfocused, label 3 - Not Answered
### Directory structure

example_article.txt - example article, please replace this with articles you wanna run eval on

example_anchor_answer_info.csv - example article anchor and answer information, please replace it with the one generated by your qud paser

### [for gpt answer base](https://github.com/lingchensanwen/QUDeval/tree/main/code/criteria2_answer_compatibility/gpt-answer-base)
<code>!python gpt_response</code> In this repo example we use gpt-3.5-turbo

### [for gpt score base](https://github.com/lingchensanwen/QUDeval/tree/main/code/criteria2_answer_compatibility/gpt-score-base)
To run this, you need to define a small set of your data to find best threshold on it, and then using this threshold to classify your main data.
In this example, we use it as whole set to compute just as an example. In your case, please define a val set and pick best threshold on it and run on test set.

To obatin best threshold run <code>!python find_best_threshold</code> and you will generate best threshold saved in best_threshold_info.txt

Then to classify your files on threshold run <code>!python eval_by_threshold</code>. 

