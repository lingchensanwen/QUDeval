# Data Collection and Annotation

This repository hosts a dataset of 2,190 Question Under Discussion (QUD) entries, carefully annotated to support further studies in the field.

## Dataset Structure 📊

The dataset includes the following headers:

- `anchor_id`
- `answer_id`
- `questions`
- `criteria1`
- `criteria2`
- `criteria3`
- `criteria4`
- `system`
- `essay_id`

## Annotation Process 🖊️

### Initial Process

Three skilled annotators — Keziah Reina, Kathryn Kazanas, and Karim Villaescusa F. — collaborated on this project. We applied the majority rule to establish the final annotation for each entry. In instances where a consensus wasn't reached, we organized a dedicated session to revisit and reconcile the divergent points of view.

### Post-submission Updates 📝

It's important to note that we revisited and adjusted certain annotations post-paper submission to address cases lacking initial agreement among the annotators. This step was vital in achieving a majority consensus and enhancing the dataset's reliability.

## Acknowledgments 🌟

A heartfelt thanks to our annotators: Keziah Reina, Kathryn Kazanas, Karim Villaescusa F. Their time and expertise have been invaluable in creating this robust dataset.

## Contact 📬

For queries or further discussions, please feel free to reach out to [Yating Wu](mailto:yating.wu@utexas.edu).

## Citation 📚

If you're utilizing our dataset in your research, we kindly ask you to cite our paper:

QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing (bibtex coming soon)
