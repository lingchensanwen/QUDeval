# QUDeval: Evaluating Questions Under Discussion Discourse Parsing

Welcome to the official repository for the EMNLP 2023 paper, "QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing."

**Authors:** [Yating Wu](http://lingchensanwen.github.io), [Ritika Mangla](https://ritikamangla01.netlify.app), [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/), and [Junyi Jessy Li](https://jessyli.com)

## Introduction üåü

QUDeval is the inaugural framework designed specifically for the automatic evaluation of Question Under Discussion (QUD) parsing. This repository hosts the essential code and metadata for QUDeval. While this project is a work in progress, we are committed to updating details and resources regularly.

## Table of Contents üìö

1. [Data Collection](#data-collection)
2. [Code for Evaluation](#code-for-evaluation)

## Data Collection üì•

The dataset associated with our research is fundamental to QUDeval. For access to the data, please send us an [email](mailto:yating.wu@utexas.edu). Once acquired, the data should be placed in the [`data`](https://github.com/lingchensanwen/QUDeval/tree/main/data) folder.

## Code for Evaluation üñ•Ô∏è

*Work in progress:* We're in the process of finalizing the scripts for data processing and evaluation. These resources will soon be available in the [`code`](https://github.com/lingchensanwen/QUDeval/tree/main/code) folder.

---

Stay tuned for more updates, and thank you for your interest in QUDeval!

