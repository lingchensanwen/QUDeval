# QUDeval: Evaluating Questions Under Discussion Discourse Parsing

Welcome to the official repository for the EMNLP 2023 paper, "QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing."

**Authors:** [Yating Wu](http://lingchensanwen.github.io), [Ritika Mangla](https://ritikamangla01.netlify.app), [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/), and [Junyi Jessy Li](https://jessyli.com)

## Introduction üåü

This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. 

## Table of Contents üìö

1. [Data Collection](#data-collection)
2. [Code for Evaluation](#code-for-evaluation)

## Data Collection üì•

The dataset associated with our research is fundamental to QUDeval. For access to the data, please send us an [email](mailto:yating.wu@utexas.edu). Once acquired, the data should be placed in the [`data`](https://github.com/lingchensanwen/QUDeval/tree/main/data) folder.

## Code for Evaluation üñ•Ô∏è

*Work in progress:* We're in the process of finalizing the scripts for data processing and evaluation. These resources will soon be available in the [`code`](https://github.com/lingchensanwen/QUDeval/tree/main/code) folder.

---

Stay tuned for more updates, and thank you for your interest in QUDeval!

