# QUDeval: Evaluating Questions Under Discussion Discourse Parsing

Welcome to the official repository for the EMNLP 2023 paper, "QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing."

**Authors:** [Yating Wu](http://lingchensanwen.github.io), [Ritika Mangla](https://ritikamangla01.netlify.app), [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/), and [Junyi Jessy Li](https://jessyli.com)


If you find our work useful or relevant to your research, please consider citing our paper. 
```bibtex
@inproceedings{wu2023qudeval,
  title={QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing},
  author={Wu, Yating and Mangla, Ritika and Durrett, Greg and Li, Junyi Jessy},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5344--5363},
  year={2023}
}

```

## Introduction üåü

This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. 

## Table of Contents üìö

0. [Code for Data Collection](#code-for-data-collection)
1. [Data Collection](#data-collection)
2. [Code for Evaluation](#code-for-evaluation)


## Code for Data Collection üìÅ

The code can be found under [this repo](https://github.com/lingchensanwen/QUD-eval-data-collection-flask)

## Data Collection üì•

The data is placed under the [`data`](https://github.com/lingchensanwen/QUDeval/tree/main/data) folder.

## Code for Evaluation üñ•Ô∏è

The code can be found under [`code`](https://github.com/lingchensanwen/QUDeval/tree/main/code) folder
